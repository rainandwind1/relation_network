{
  "artifacts": [],
  "command": "my_main",
  "experiment": {
    "base_dir": "/home/zpp/graduation_design/pymarl_base/pymarl-master/src",
    "dependencies": [
      "munch==2.5.0",
      "numpy==1.19.5",
      "PyYAML==5.4.1",
      "sacred==0.8.2",
      "torch==1.7.1"
    ],
    "mainfile": "main.py",
    "name": "pymarl",
    "repositories": [],
    "sources": [
      [
        "main.py",
        "_sources/main_81faf643d23e29cf87c8d61d96135469.py"
      ],
      [
        "run.py",
        "_sources/run_24181db673ab740d035297efe41d3a86.py"
      ],
      [
        "utils/__init__.py",
        "_sources/__init___d41d8cd98f00b204e9800998ecf8427e.py"
      ],
      [
        "utils/logging.py",
        "_sources/logging_2c814fe775935963fe055f6f37191d6e.py"
      ]
    ]
  },
  "fail_trace": [
    "Traceback (most recent call last):\n",
    "  File \"/home/zpp/anaconda3/envs/Smac/lib/python3.8/site-packages/sacred/config/captured_function.py\", line 42, in captured_function\n    result = wrapped(*args, **kwargs)\n",
    "  File \"src/main.py\", line 35, in my_main\n    run(_run, config, _log)\n",
    "  File \"/home/zpp/graduation_design/pymarl_base/pymarl-master/src/run.py\", line 49, in run\n    run_sequential(args=args, logger=logger)\n",
    "  File \"/home/zpp/graduation_design/pymarl_base/pymarl-master/src/run.py\", line 186, in run_sequential\n    learner.load_models(model_path)\n",
    "  File \"/home/zpp/graduation_design/pymarl_base/pymarl-master/src/learners/q_learner.py\", line 194, in load_models\n    self.mixer.load_state_dict(th.load(\"{}/mixer.th\".format(path), map_location=lambda storage, loc: storage))\n",
    "  File \"/home/zpp/anaconda3/envs/Smac/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "RuntimeError: Error(s) in loading state_dict for Multihead_QMixer:\n\tsize mismatch for multi_head_module.q_net.weight: copying a param with shape torch.Size([20, 176]) from checkpoint, the shape in current model is torch.Size([30, 176]).\n\tsize mismatch for multi_head_module.q_net.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for multi_head_module.k_net.weight: copying a param with shape torch.Size([20, 176]) from checkpoint, the shape in current model is torch.Size([30, 176]).\n\tsize mismatch for multi_head_module.k_net.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for multi_head_module.v_net.weight: copying a param with shape torch.Size([20, 176]) from checkpoint, the shape in current model is torch.Size([30, 176]).\n\tsize mismatch for multi_head_module.v_net.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([30]).\n\tsize mismatch for multi_head_module.multihead_net.in_proj_weight: copying a param with shape torch.Size([60, 20]) from checkpoint, the shape in current model is torch.Size([90, 30]).\n\tsize mismatch for multi_head_module.multihead_net.in_proj_bias: copying a param with shape torch.Size([60]) from checkpoint, the shape in current model is torch.Size([90]).\n\tsize mismatch for multi_head_module.multihead_net.out_proj.weight: copying a param with shape torch.Size([20, 20]) from checkpoint, the shape in current model is torch.Size([30, 30]).\n\tsize mismatch for multi_head_module.multihead_net.out_proj.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([30]).\n"
  ],
  "heartbeat": "2022-04-30T10:25:27.004691",
  "host": {
    "ENV": {},
    "cpu": "Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz",
    "gpus": {
      "driver_version": "460.39",
      "gpus": [
        {
          "model": "GeForce RTX 3090",
          "persistence_mode": false,
          "total_memory": 24268
        },
        {
          "model": "GeForce RTX 3090",
          "persistence_mode": false,
          "total_memory": 24268
        },
        {
          "model": "GeForce RTX 3090",
          "persistence_mode": false,
          "total_memory": 24268
        },
        {
          "model": "GeForce RTX 3090",
          "persistence_mode": false,
          "total_memory": 24268
        }
      ]
    },
    "hostname": "admin001-SYS-7049GP-TRT",
    "os": [
      "Linux",
      "Linux-5.8.0-49-generic-x86_64-with-glibc2.10"
    ],
    "python_version": "3.8.5"
  },
  "meta": {
    "command": "my_main",
    "options": {
      "--beat-interval": null,
      "--capture": null,
      "--comment": null,
      "--debug": false,
      "--enforce_clean": false,
      "--file_storage": null,
      "--force": false,
      "--help": false,
      "--loglevel": null,
      "--mongo_db": null,
      "--name": null,
      "--pdb": false,
      "--print-config": false,
      "--priority": null,
      "--queue": false,
      "--s3": null,
      "--sql": null,
      "--tiny_db": null,
      "--unobserved": false,
      "COMMAND": null,
      "UPDATE": [
        "env_args.map_name=MMM2"
      ],
      "help": false,
      "with": true
    }
  },
  "resources": [],
  "result": null,
  "start_time": "2022-04-30T10:25:23.173569",
  "status": "FAILED",
  "stop_time": "2022-04-30T10:25:27.006347"
}